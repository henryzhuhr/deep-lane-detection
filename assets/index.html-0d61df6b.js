import{_ as c}from"./lane-define-f9a0e76f.js";import{_ as p,r as l,o as d,c as u,a as n,b as e,d as a,e as r,w as i,f as t}from"./app-909dbfb3.js";const h={},b=t('<h1 id="车道线检测深度学习方法" tabindex="-1"><a class="header-anchor" href="#车道线检测深度学习方法" aria-hidden="true">#</a> 车道线检测深度学习方法</h1><ul><li><a href="#%E8%BD%A6%E9%81%93%E7%BA%BF%E6%A3%80%E6%B5%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95">车道线检测深度学习方法</a><ul><li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li><li><a href="#%E9%A1%B9%E7%9B%AE">项目</a></li><li><a href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83">配置环境</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87">数据集准备</a></li><li><a href="#%E8%AE%AD%E7%BB%83">训练</a></li><li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li><li><a href="#%E9%83%A8%E7%BD%B2">部署</a><ul><li><a href="#1-onnx-%E9%83%A8%E7%BD%B2">1. ONNX 部署</a></li><li><a href="#2-tensorrt-%E9%83%A8%E7%BD%B2">2. TensorRT 部署</a><ul><li><a href="#%E5%AE%89%E8%A3%85-tensorrt-%E7%8E%AF%E5%A2%83">安装 TensorRT 环境</a></li><li><a href="#1-tensorrt-%E6%8E%A8%E7%90%86-onnx-%E6%A8%A1%E5%9E%8B">1. TensorRT 推理 ONNX 模型</a></li><li><a href="#2-tensorrt-%E6%8E%A8%E7%90%86-engine-%E6%A8%A1%E5%9E%8B">2. TensorRT 推理 Engine 模型</a></li></ul></li></ul></li><li><a href="#%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">问题与解决方案</a><ul><li><a href="#tensor-rt-engine-%E8%BD%AC%E6%8D%A2%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98">Tensor RT Engine 转换过程中的问题</a></li></ul></li><li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li></ul></li></ul><h2 id="任务" tabindex="-1"><a class="header-anchor" href="#任务" aria-hidden="true">#</a> 任务</h2><p><img src="'+c+'" alt="culane"></p><p>车道线检测 (Lane Detetction) ：需要将视频中出现的车道线检测出来。任务要求如下:</p><ol><li>查询车道线检测的相关综述，包括<code>传统视觉</code>和<code>深度学习</code>的方案</li><li>通常需要检测四条车道线：左车道、自车道、右车道。</li><li>需要判断双黄线、白色虚线、白色实线</li><li>如果车道线终止，需要判断出结束位置</li><li>车道线需要拟合成曲线，检测的车道线要求稳定，不能发生连续帧之间突变的情况</li></ol><h2 id="项目" tabindex="-1"><a class="header-anchor" href="#项目" aria-hidden="true">#</a> 项目</h2>',7),m={href:"https://github.com/cfzd/Ultra-Fast-Lane-Detection",target:"_blank",rel:"noopener noreferrer"},v=t(`<h2 id="配置环境" tabindex="-1"><a class="header-anchor" href="#配置环境" aria-hidden="true">#</a> 配置环境</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>conda create <span class="token parameter variable">-n</span> lanedet <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span> <span class="token parameter variable">-y</span>
conda activate lanedet
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,2),_={href:"https://pytorch.org",target:"_blank",rel:"noopener noreferrer"},g=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> torch torchvision --index-url https://download.pytorch.org/whl/cu118
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>安装其他依赖</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="数据集准备" tabindex="-1"><a class="header-anchor" href="#数据集准备" aria-hidden="true">#</a> 数据集准备</h2><p>车道线检测常用的公开数据集:</p>`,5),k={href:"https://xingangpan.github.io/projects/CULane.html",target:"_blank",rel:"noopener noreferrer"},f={href:"https://www.kaggle.com/datasets/manideep1108/tusimple?resource=download",target:"_blank",rel:"noopener noreferrer"},T={href:"https://github.com/SoulmateB/CurveLanes",target:"_blank",rel:"noopener noreferrer"},E={href:"https://xingangpan.github.io/projects/CULane.html",target:"_blank",rel:"noopener noreferrer"},x=t(`<p>无论是使用 <strong>CULane 数据集</strong>还是<strong>自制数据集</strong>，都需要设置环境变量 <code>$CULANEROOT</code></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CULANEROOT</span><span class="token operator">=</span>/path/to/culane
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="训练" tabindex="-1"><a class="header-anchor" href="#训练" aria-hidden="true">#</a> 训练</h2><p>首先，修改配置文件 <code>configs/culane.py</code> 中重要的参数（复制一份配置文件到 <code>temp/culane.py</code> 修改，而不是修改原始的配置文件），包括：</p>`,4),A=t("<li><code>data_root</code> 是 CULane 数据集路径</li><li><code>log_path</code> 是输出的模型路径，这里默认为 <code>tmps</code></li><li>训练中的超参数，比如 <code>epoch</code>,<code>batch_size</code>,<code>steps</code> 等</li>",3),N=n("code",null,"resume",-1),R={href:"https://drive.google.com/file/d/1zXBRTw50WOzvUp6XKsi8Zrk3MUC3uFuq/view?usp=sharing",target:"_blank",rel:"noopener noreferrer"},y={href:"https://pan.baidu.com/s/19Ig0TrV8MfmFTyCvbSa4ag?pwd=w9tw",target:"_blank",rel:"noopener noreferrer"},D=t(`<p>确认配置文件无误后启动训练脚本，需要修改脚本 <code>temp/culane.py</code> 内</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 train.py temp/culane.py <span class="token comment"># 单 GPU 训练</span>
<span class="token function">bash</span> scripts/train-dist.sh      <span class="token comment"># 多 GPU 训练</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="测试" tabindex="-1"><a class="header-anchor" href="#测试" aria-hidden="true">#</a> 测试</h2><p>训练完成后，需要对模型进行测试，这里是对 <code>$CULANEROOT</code> 进行测试，如果只希望对单张图像进行测试，在下文提到。需要修改 <code>temp/culane.py</code> 中 <code>test_model</code>(待测试的模型权重文件) 和 <code>test_work_dir</code>(测试结果输出的目录，默认<code>tmps</code>)，然后运行</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 test.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>单张图像测试，需要配置的内容与上述结果相同，但是需要修改文件中 <code>test_img = &quot;datasets/CULane/images/04980.jpg&quot;</code> 为指定的图像</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 infer-torch.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="部署" tabindex="-1"><a class="header-anchor" href="#部署" aria-hidden="true">#</a> 部署</h2><p>部署分为两种方式：</p><ol><li><strong>ONNX 部署</strong>：将 pytorch 模型转化为 ONNX 模型，然后使用 ONNXRuntime 进行推理</li><li><strong>TensorRT 部署</strong>：：将 pytorch 模型转化为 ONNX/Engine 模型，然后使用 TensorRT 进行推理</li></ol><p>这里给出一种推理速度参考</p><table><thead><tr><th style="text-align:center;">推理方式</th><th style="text-align:center;">平均推理时间</th></tr></thead><tbody><tr><td style="text-align:center;">Pytorch</td><td style="text-align:center;">30.9649 ms</td></tr><tr><td style="text-align:center;">ONNXRuntime</td><td style="text-align:center;">19.9175 ms</td></tr><tr><td style="text-align:center;">TensorRT Engine</td><td style="text-align:center;">6.5350 ms</td></tr></tbody></table>`,12),B={href:"https://developer.nvidia.com/embedded/jetson-nano-developer-kit",target:"_blank",rel:"noopener noreferrer"},O={href:"https://developer.nvidia.com/embedded/jetpack-sdk-461",target:"_blank",rel:"noopener noreferrer"},C=n("li",null,[n("strong",null,"OS"),e(": Ubuntu 18.04, Linux kernel 4.9")],-1),U={href:"https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-821/quick-start-guide/index.html",target:"_blank",rel:"noopener noreferrer"},L=n("strong",null,"TensorRT 8.2.1",-1),I=n("li",null,[n("strong",null,"cuDNN 8.2.1")],-1),P={href:"https://docs.nvidia.com/cuda/archive/10.2/cuda-toolkit-release-notes/index.html#title-new-features",target:"_blank",rel:"noopener noreferrer"},H=n("strong",null,"CUDA 10.2",-1),w=t(`<h3 id="_1-onnx-部署" tabindex="-1"><a class="header-anchor" href="#_1-onnx-部署" aria-hidden="true">#</a> 1. ONNX 部署</h3><p>修改配置文件 <code>temp/culane.py</code> 中 <code>fintune</code> 为 pytorch 模型的权重，运行脚本后会在相同目录下生成同名的 <code>.onnx</code> 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 export.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>得到 <code>.onnx</code> 文件后，修改推理脚本 <code>infer-onnx.py</code> 中的 onnx 模型权重 <code>onnx_file</code> 和待测试视频 <code>video</code>，然后运行如下</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 infer-onnx.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_2-tensorrt-部署" tabindex="-1"><a class="header-anchor" href="#_2-tensorrt-部署" aria-hidden="true">#</a> 2. TensorRT 部署</h3>`,6),$={href:"https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8",target:"_blank",rel:"noopener noreferrer"},M=n("ol",null,[n("li",null,"使用 TensorRT 推理 ONNX 模型"),n("li",null,"将 ONNX 转化为 Engine 模型并使用 TensorRT 推理")],-1),S={href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html",target:"_blank",rel:"noopener noreferrer"},X={href:"https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html",target:"_blank",rel:"noopener noreferrer"},z={href:"https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/index.html",target:"_blank",rel:"noopener noreferrer"},J=n("h4",{id:"安装-tensorrt-环境",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#安装-tensorrt-环境","aria-hidden":"true"},"#"),e(" 安装 TensorRT 环境")],-1),V=n("p",null,"必须 Ubuntu + Nvidia GPU 环境",-1),q={href:"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar",target:"_blank",rel:"noopener noreferrer"},F={href:"https://developer.nvidia.cn/login",target:"_blank",rel:"noopener noreferrer"},Y=n("p",null,[n("strong",null,"安装 CUDA")],-1),j={href:"https://developer.nvidia.com/cuda-toolkit-archive",target:"_blank",rel:"noopener noreferrer"},G=n("code",null,"deb(local)",-1),W=n("strong",null,"注意",-1),K={href:"https://developer.nvidia.com/cuda-10.2-download-archive",target:"_blank",rel:"noopener noreferrer"},Z={href:"https://developer.nvidia.com/cuda-11-4-0-download-archive",target:"_blank",rel:"noopener noreferrer"},Q=n("p",null,[n("strong",null,"安装 cuDNN")],-1),nn={href:"https://developer.nvidia.com/rdp/cudnn-archive",target:"_blank",rel:"noopener noreferrer"},en=n("strong",null,"CUDA 版本",-1),an=n("strong",null,"系统架构",-1),sn=n("strong",null,"Tar",-1),tn={href:"https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.1.32/10.2_06072021/cudnn-10.2-linux-x64-v8.2.1.32.tgz",target:"_blank",rel:"noopener noreferrer"},on=n("code",null,"cuDNN v8.2.1, for CUDA 10.2, for Linux (x86)",-1),ln={href:"https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.1.32/11.3_06072021/cudnn-11.3-linux-x64-v8.2.1.32.tgz",target:"_blank",rel:"noopener noreferrer"},rn=n("code",null,"cuDNN v8.2.1, for CUDA 11.x, for Linux (x86)",-1),cn={href:"https://developer.nvidia.com/downloads/compute/cudnn/secure/8.9.0/local_installers/11.8/cudnn-linux-x86_64-8.9.0.131_cuda11-archive.tar.xz/",target:"_blank",rel:"noopener noreferrer"},pn=n("code",null,"cuDNN8.9.0 + CUDA 11.8 + Linux x86_64 (Tar)",-1),dn=t('<p>下载后得到压缩包 <code>cudnn-${version}.tar.xz</code> ，将压缩包解压后得到同名的目录，但是8.2.1 解压后得到的是 cuda 目录，建议改名</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">tar</span> <span class="token parameter variable">-xf</span> cudnn-<span class="token variable">${version}</span>.tar.xz\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>',2),un=n("p",null,[n("strong",null,"安装 TensorRT")],-1),hn={href:"https://developer.nvidia.com/tensorrt-getting-started",target:"_blank",rel:"noopener noreferrer"},bn=n("em",null,"Download Now",-1),mn={href:"https://developer.nvidia.com/compute/machine-learning/tensorrt/secure/8.2.1/tars/tensorrt-8.2.1.8.linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz",target:"_blank",rel:"noopener noreferrer"},vn=n("code",null,"TensorRT 8.2 GA for Linux x86_64 and CUDA 11.0-5 TAR Package",-1),_n=t('<p>下载后得到压缩包 <code>TensorRT-${version}.tar.gz</code> ，将压缩包解压后得到同名的目录</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">tar</span> <span class="token parameter variable">-xf</span> TensorRT-<span class="token variable">${version}</span>.tar.gz\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>',2),gn=t(`<p>在系统环境变量中添加 CUDA, cuDNN,TensorRT 相关的路径(修改为真实路径<code>$xxx_HOME</code>)。添加完成后，<code>source ~/.bashrc</code> 使环境变量生效</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token comment"># ------ CUDA ------</span>
<span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token number">11.8</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-<span class="token variable">\${CUDA_VERSION}</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/lib64
<span class="token comment"># ------ cuDNN 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDNN_HOME</span><span class="token operator">=</span>path/to/cudnn-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span> 
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDNN_HOME</span>/lib64 <span class="token comment"># 新版本为 lib 目录，建议自己检查一下</span>
<span class="token comment"># ------ TensorRT 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_HOME</span><span class="token operator">=</span>path/to/TensorRT-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/bin
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>安装 python相关环境</li></ol>`,3),kn={href:"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip",target:"_blank",rel:"noopener noreferrer"},fn={href:"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip",target:"_blank",rel:"noopener noreferrer"},Tn=n("code",null,"tensorrt",-1),En=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 根据 python 版本选择对应的 .whl 文件</span>
python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> <span class="token variable">$TENSORRT_HOME</span>/python/tensorrt-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>.whl
python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pycuda<span class="token operator">&gt;=</span><span class="token number">2020.1</span> <span class="token comment"># 已写入 requirements.txt</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_1-tensorrt-推理-onnx-模型" tabindex="-1"><a class="header-anchor" href="#_1-tensorrt-推理-onnx-模型" aria-hidden="true">#</a> 1. TensorRT 推理 ONNX 模型</h4>`,2),xn={href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_topics",target:"_blank",rel:"noopener noreferrer"},An=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h4 id="_2-tensorrt-推理-engine-模型" tabindex="-1"><a class="header-anchor" href="#_2-tensorrt-推理-engine-模型" aria-hidden="true">#</a> 2. TensorRT 推理 Engine 模型</h4>`,2),Nn={href:"https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-821/quick-start-guide/index.html#export-from-pytorch",target:"_blank",rel:"noopener noreferrer"},Rn={href:"https://github.com/NVIDIA/TensorRT/tree/release/8.2",target:"_blank",rel:"noopener noreferrer"},yn={href:"https://github.com/NVIDIA/TensorRT/blob/release/8.2/samples/python/efficientnet/requirements.txt",target:"_blank",rel:"noopener noreferrer"},Dn=n("code",null,"requirements.txt",-1),Bn=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token assign-left variable">onnx</span><span class="token operator">==</span><span class="token number">1.9</span>.0 <span class="token comment"># 已写入 requirements.txt</span>
python3 export.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>在转换之前，确保有 <code>**-INT32.onnx</code> 模型，因为 TensorRT 支持 INT32 而不支持 INT64。如果没有，可以使用 <code>onnxsim</code> 工具进行转换</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> onnxsim weights/culane_18.onnx weights/culane_18-sim.onnx
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>上述操作都可以在电脑上完成，但是如果需要在 Jetson Nano 上推理，则需要将 onnx 转换为 Engine 模型需要在 Jetson Nano 上完成，否则会出现<a href="#issuse-tensorrt-engine_incompatible_device">&quot;<em>不匹配设备的报错</em>&quot;</a>，但是后面的步骤可以在电脑上测试没有问题再在 Jetson Nano 上部署。</p><p>Jetpack 自带的库在 <code>/usr/src</code>，因此在 Jetson Nano 的系统环境变量中添加如下内容，然后<code>source ~/.bashrc</code> 使环境变量生效</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token comment"># ------ CUDA 10.2 ------</span>
<span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token number">10.2</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-<span class="token variable">\${CUDA_VERSION}</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/lib64
<span class="token comment"># ------ TensorRT 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_HOME</span><span class="token operator">=</span>/usr/src/tensorrt
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/bin
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,6),On=n("code",null,"**-INT32.onnx",-1),Cn=n("code",null,"**-INT32.engine",-1),Un=n("a",{href:"#TensorRT-Engine-%E8%BD%AC%E6%8D%A2%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"},'"Tensor RT Engine 转换过程中的问题"',-1),Ln=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 export.py configs/culane.py
trtexec <span class="token parameter variable">--verbose</span> <span class="token parameter variable">--fp16</span> <span class="token punctuation">\\</span>
  <span class="token parameter variable">--onnx</span><span class="token operator">=</span>weights/culane_18-INT32.onnx <span class="token punctuation">\\</span>
  <span class="token parameter variable">--saveEngine</span><span class="token operator">=</span>weights/culane_18-INT32.engine
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><code>--workspace=N</code>: Set workspace size in megabytes (default = 16)</li><li><code>--fp16</code>: Enable fp16 precision, in addition to fp32 (default = disabled)</li><li><code>--int8</code>: Enable int8 precision, in addition to fp32 (default = disabled)</li><li><code>--verbose</code>: Use verbose logging (default = false)</li><li><code>--exportTimes=&lt;file&gt;</code> :Write the timing results in a json file (default = disabled)</li><li><code>--exportOutput=&lt;file&gt;</code>: Write the output tensors to a json file (default = disabled)</li><li><code>--exportProfile=&lt;file&gt;</code>: Write the profile information per layer in a json file (default = disabled)</li></ul>`,2),In={href:"https://github.com/NVIDIA-AI-IOT/torch2trt",target:"_blank",rel:"noopener noreferrer"},Pn=n("code",null,"torch2trt",-1),Hn=n("code",null,"PyTroch -> Engine",-1),wn=n("a",{href:"#issuse-tensorrt-engine_incompatible_device"},"不匹配设备的报错",-1),$n=n("code",null,"PyTroch -> ONNX",-1),Mn=n("code",null,"ONNX",-1),Sn=n("code",null,"trtexec",-1),Xn=n("code",null,"ONNX -> Engine",-1),zn=t(`<p>运行前需要安装 <code>pycuda</code></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> pycuda<span class="token operator">&gt;=</span><span class="token number">2020.1</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>然后修改 <code>deploy/infer-trtEngine.py</code> 中的 <code>TRT_MODEL_PATH</code> 为 <code>**-INT32.engine</code> 的路径，运行</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> deploy
python3 infer-trtEngine.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="问题与解决方案" tabindex="-1"><a class="header-anchor" href="#问题与解决方案" aria-hidden="true">#</a> 问题与解决方案</h2><h3 id="tensor-rt-engine-转换过程中的问题" tabindex="-1"><a class="header-anchor" href="#tensor-rt-engine-转换过程中的问题" aria-hidden="true">#</a> Tensor RT Engine 转换过程中的问题</h3>`,6),Jn=t(`<p>报错如下</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token punctuation">[</span>07/13/2023-11:01:12<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> Error<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>: <span class="token punctuation">[</span>caskUtils.cpp::trtSmToCask::147<span class="token punctuation">]</span> Error Code <span class="token number">1</span>: Internal Error <span class="token punctuation">(</span>Unsupported SM: 0x809<span class="token punctuation">)</span>
<span class="token punctuation">[</span>07/13/2023-11:01:12<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> Error<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>: <span class="token punctuation">[</span>builder.cpp::buildSerializedNetwork::609<span class="token punctuation">]</span> Error Code <span class="token number">2</span>: Internal Error <span class="token punctuation">(</span>Assertion enginePtr <span class="token operator">!=</span> nullptr failed. <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,2),Vn={href:"https://github.com/NVIDIA/TensorRT/issues/2727#issuecomment-1492809565",target:"_blank",rel:"noopener noreferrer"},qn=n("code",null,"Unsupported SM",-1),Fn=t(`<li><p>报错如下<span id="issuse-tensorrt-engine_incompatible_device"></span></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token punctuation">[</span>07/14/2023-11:41:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> <span class="token number">6</span>: The engine plan <span class="token function">file</span> is generated on an incompatible device, expecting compute <span class="token number">5.3</span> got compute <span class="token number">6.1</span>, please rebuild.
<span class="token punctuation">[</span>07/14/2023-11:41:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> <span class="token number">4</span>: <span class="token punctuation">[</span>runtime.cpp::deserializeCudaEngine::50<span class="token punctuation">]</span> Error Code <span class="token number">4</span>: Internal Error <span class="token punctuation">(</span>Engine deserialization failed.<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>这是由于 Engine 模型不是在 Jetson nano 上生成的，在 Jetson nano 上重新生成 Engine 模型即可</p></li>`,1),Yn=n("h2",{id:"参考资料",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#参考资料","aria-hidden":"true"},"#"),e(" 参考资料")],-1),jn={href:"https://github.com/Turoad/lanedet",target:"_blank",rel:"noopener noreferrer"},Gn={href:"https://zhuanlan.zhihu.com/p/353339637",target:"_blank",rel:"noopener noreferrer"},Wn={href:"https://github.com/qinnzou/Robust-Lane-Detection.git",target:"_blank",rel:"noopener noreferrer"};function Kn(Zn,Qn){const s=l("ExternalLinkIcon"),o=l("RouterLink");return d(),u("div",null,[b,n("p",null,[e("项目是基于"),n("a",m,[e("Ultra Fast Lane Detection"),a(s)])]),v,n("p",null,[e("安装"),n("a",_,[e("Pytorch"),a(s)]),e("依赖，需要 CUDA 11.8")]),g,n("ul",null,[n("li",null,[n("a",k,[e("CULane"),a(s)])]),n("li",null,[n("a",f,[e("TuSimple"),a(s)])]),n("li",null,[n("a",T,[e("CurveLanes"),a(s)]),e(":华为数据集 这里我们选择 "),n("a",E,[e("CULane"),a(s)]),e(" 作为参考进行实验、制作自己的数据集")])]),r(` 作者提供了[分割工具](https://github.com/XingangPan/seg_label_generate.git)
项目实现 [PytorchAutoDrive](https://github.com/voldemortX/pytorch-auto-drive) `),n("p",null,[e("CULane 数据集处理和制作参考"),a(o,{to:"/dataset-culane.html"},{default:i(()=>[e("CULane文档")]),_:1})]),x,n("ul",null,[A,n("li",null,[N,e(" 是预训练权重，如果需要加载预训练权重，可以下载官方的预训练权重: "),n("a",R,[e("GoogleDrive"),a(s)]),e("/"),n("a",y,[e("BaiduDrive"),a(s)]),e("，官方的预训练权重基于 resnet18，如果希望替换成其他 backbone，需要从头训练")])]),D,n("p",null,[e("TensorRT 部署在 Jetson 上需要考虑系统环境，例如 "),n("a",B,[e("Jetson Nano"),a(s)]),e(" 系统中包含"),n("a",O,[e("JetPack 4.6.1"),a(s)]),e(":")]),n("ul",null,[C,n("li",null,[n("a",U,[L,a(s)])]),I,n("li",null,[n("a",P,[H,a(s)])])]),w,n("p",null,[e("参考 "),n("a",$,[e("TensorRT 官方文档"),a(s)]),e("，部署有两种方式：")]),M,r(" https://zhuanlan.zhihu.com/p/527238167 "),n("p",null,[e("TensorRT官方文档 "),n("a",S,[e('"NVIDIA Official Documentation"'),a(s)])]),n("ul",null,[n("li",null,[n("a",X,[e("TensorRT Python API."),a(s)])]),n("li",null,[n("a",z,[e("TensorRT C++ API."),a(s)])])]),J,V,n("ol",null,[n("li",null,[e("安装 CUDA、cuDNN、TensorRT，参考"),n("a",q,[e("官方文档"),a(s)]),e("，需要注册 "),n("a",F,[e("Nvidia 账号"),a(s)]),e("，并登陆")])]),n("ul",null,[n("li",null,[Y,n("p",null,[e("进入"),n("a",j,[e("CUDA 下载页面"),a(s)]),e("并选择需要的版本，根据电脑的配置依次选择各项，最后选择 "),G,e("安装方式，将出现的命令依次复制到终端中执行。")]),n("p",null,[W,e(": JetPack 4.6.1 "),n("a",K,[e("CUDA 10.2"),a(s)]),e("，但是 10.2 不支持 Ubuntu20，所以下载 "),n("a",Z,[e("CUDA 11.4"),a(s)])])]),n("li",null,[Q,n("p",null,[e("进入"),n("a",nn,[e("cuDNN 下载页面"),a(s)]),e("，根据 "),en,e("和"),an,e("选择对应的版本的 "),sn,e("文件，下载以下内文件：")]),n("ul",null,[n("li",null,[n("a",tn,[on,a(s)]),e(": 安装在 Jetson Nano 上用于转换 Engine 模型")]),n("li",null,[n("a",ln,[rn,a(s)]),e(": 安装在电脑上用于转换 Engine 模型和测试，这一步是为了在电脑上编写推理代码和测试，如果推理代码已经写好，可以不用安装")]),n("li",null,[n("a",cn,[pn,a(s)]),e(": 最新版本")])]),dn]),n("li",null,[un,n("p",null,[e("进入"),n("a",hn,[e("下载页面"),a(s)]),e("，点击 "),bn,e(" 的入口，选择版本的 Tar 文件下载：")]),n("ul",null,[n("li",null,[n("a",mn,[vn,a(s)])])]),_n])]),gn,n("p",null,[e("参考官方文档 "),n("a",kn,[e('"Python Package Index Installation"'),a(s)]),e(" 安装 python 的"),n("a",fn,[Tn,a(s)])]),En,n("p",null,[e("参考官方"),n("a",xn,[e("Python API"),a(s)])]),An,n("p",null,[e("参考官方文档"),n("a",Nn,[e('"Exporting To ONNX From PyTorch"/"Converting ONNX To A TensorRT Engine"'),a(s)]),e("这种方式的部署流程是：首先将 ONNX 模型转化为 TensorRT Engine 模型，再使用 TensorRT API 推理 Engine 模型")]),n("p",null,[e("转换过程中需要考虑到 TensorRT 支持的 ONNX 版本，具体可以参考 "),n("a",Rn,[e("TensorRT(8.2.1) 源码"),a(s)]),e("中的 "),n("a",yn,[Dn,a(s)]),e(" 文件，所以需要重新安装 ONNX 再重新转换")]),Bn,n("p",null,[e("使用官方提供的转换工具进行转换，如果 TensorRT 环境配置正确，将 "),On,e(" 转化为 "),Cn,e("。转换过程中可能出现的bug以及解决方案记录在"),Un,e("中，这里提供一个"),a(o,{to:"/onnx2engine.html"},{default:i(()=>[e("输出细节参考")]),_:1})]),Ln,n("blockquote",null,[n("p",null,[e("实际上，NV 官方提供了 "),n("a",In,[Pn,a(s)]),e(" 转换工具可以直接完成 "),Hn,e("，但是在 Jetson Nano 上部署的时候，需要在 Jetson Nano 上完成模型转换，否则在实际使用时会出现"),wn,e("，那么将完整的 torch 项目直接安装在 Jetson Nano 上可能会遇到很多问题，因此，这里先在电脑上完成 "),$n,e(" 转换，然后将转换好的 "),Mn,e(" 复制到 Jetson Nano 上使用 "),Sn,e(" 完成 "),Xn,e(" 转换，可以避免在 Jetson Nano 上安装 torch 环境和项目的一些其他环境。")])]),zn,n("ul",null,[n("li",null,[Jn,n("p",null,[e("参考 "),n("a",Vn,[e("TensorRT #2727"),a(s)]),e("，"),qn,e(" 表示该版本的 TensorRT 不支持当前的 GPU 的 SM（SM是流媒体多处理器(Streaming Multiprocessor)，RTX40系列具有与以前的GPU系列不同的SM架构），需要升级 TensorRT 版本，或者选择比如RTX3080的GPU，TensorRT 8.5.1.7以上版本支持RTX40系SM")])]),Fn]),Yn,n("ul",null,[n("li",null,[n("p",null,[e("车道线检测项目"),n("a",jn,[e("lanedet"),a(s)])])]),n("li",null,[n("p",null,[e("参考 百度 Apollo 项目的"),n("a",Gn,[e("车道线检测方法"),a(s)]),e("，使用深度学习方法进行车道线检测。")])]),n("li",null,[n("p",null,[e("CNN + LSTM "),n("a",Wn,[e("Robust-Lane-Detection"),a(s)])])])])])}const ae=p(h,[["render",Kn],["__file","index.html.vue"]]);export{ae as default};
